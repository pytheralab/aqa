{
    "model":"/weights/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
    "disable_log_requests": true,
    "gpu_memory_utilization": 0.85,
    "max_model_len": 3392,
    "enforce_eager": true,
    "dtype": "float16"
}