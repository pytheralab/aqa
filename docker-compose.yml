volumes:
  predown_models:
  deepseek_data:

services:
  rag:
    image: heronq02/tritonserver:24.12-cpuonly
    volumes:
      - predown_models:/models
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -n '' > /dev/tcp/127.0.0.1/8000"]
      interval: 30s
      timeout: 2m
      retries: 2

  llm:
    image: presencesw/meta-llama-3-8b-instruct-gguf:24.12
    volumes:
      - ./models:/models
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -n '' > /dev/tcp/127.0.0.1/8000"]
      interval: 60s
      timeout: 2m
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    command: >
      bash -c """pip install torch 'gguf>=0.10' && tritonserver --model-repository=/models"""
  
  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - ${PWD}/qdrant_hub:/qdrant/storage
    ports:
      - "1999:6333"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333"]
      interval: 10s
      timeout: 5s
      retries: 2

  
  dsclient:
    image: heronq02/dsclient:24.12
    volumes:
      - predown_models:/predown_models
      - ./llm_gradio.py:/llm_gradio.py
      - ./mainapi.py:/mainapi.py
      - ./scripts:/scripts
      - ./src:/src
    env_file:
      - .env
    tty: true
    links:
      - rag:rag
      - llm:llm
    depends_on:
      rag:
        condition: service_healthy
      llm:
        condition: service_healthy

    ports:
      - "2222:8000"
      - "2223:7860"
    command: >
      bash -c """bash /scripts/run.sh"""

